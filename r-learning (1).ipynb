{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":370089,"sourceType":"datasetVersion","datasetId":902}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:53.886751Z","iopub.execute_input":"2025-10-30T10:21:53.887155Z","iopub.status.idle":"2025-10-30T10:21:53.896753Z","shell.execute_reply.started":"2025-10-30T10:21:53.887117Z","shell.execute_reply":"2025-10-30T10:21:53.896146Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/lending-club/rejected_2007_to_2018Q4.csv.gz\n/kaggle/input/lending-club/accepted_2007_to_2018Q4.csv.gz\n/kaggle/input/lending-club/accepted_2007_to_2018q4.csv/accepted_2007_to_2018Q4.csv\n/kaggle/input/lending-club/rejected_2007_to_2018q4.csv/rejected_2007_to_2018Q4.csv\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Offline RL pipeline for LendingClub loan approval (Kaggle-ready)\n# File: Offline_RL_LendingClub.py\n# Notebook-style script. Run on Kaggle (or your local env). Assumes you uploaded the LendingClub CSV as 'lending_club_loan_two.csv'\n\n\"\"\"\nOverview\n--------\nThis notebook frames loan approval as an *offline reinforcement learning* problem (one-step MDP / contextual bandit):\n- State (s): preprocessed applicant features\n- Action (a): {0: Deny, 1: Approve}\n- Reward (r):\n    * If action == 0 (Deny): reward = 0\n    * If action == 1 and loan fully paid: reward = +(loan_amnt * int_rate)\n    * If action == 1 and loan defaulted: reward = -loan_amnt\n\nWe use d3rlpy (a modern offline RL library) and the CQL (Conservative Q-Learning) algorithm.\nWe estimate the learned policy value with Fitted Q Evaluation (FQE) implemented in d3rlpy.\n\nHow to use on Kaggle\n---------------------\n1. Upload dataset file to the Kaggle notebook (Data pane) and name it `lending_club_loan_two.csv`.\n2. Run this notebook. It will: install dependencies, preprocess, create an offline dataset, train CQL, and compute Estimated Policy Value (EPV) using FQE.\n\nNotes\n-----\n- The script treats each loan instance as a single-step episode (terminal=True).\n- The reward engineering follows the user's specification. You may tune reward scaling (e.g., divide by 1e3) to stabilize learning.\n\n\"\"\"\n\n# ----------------------\n# Install dependencies\n# ----------------------\n# On Kaggle you can run pip install in a cell. Uncomment the pip install lines if needed.\n\n# !pip install d3rlpy==2.3.1\n# !pip install xgboost\n# !pip install category_encoders\n# !pip install pandas scikit-learn \"fsspec>=0.8.4\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:53.897917Z","iopub.execute_input":"2025-10-30T10:21:53.898094Z","iopub.status.idle":"2025-10-30T10:21:55.464235Z","shell.execute_reply.started":"2025-10-30T10:21:53.898081Z","shell.execute_reply":"2025-10-30T10:21:55.463440Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"\"\\nOverview\\n--------\\nThis notebook frames loan approval as an *offline reinforcement learning* problem (one-step MDP / contextual bandit):\\n- State (s): preprocessed applicant features\\n- Action (a): {0: Deny, 1: Approve}\\n- Reward (r):\\n    * If action == 0 (Deny): reward = 0\\n    * If action == 1 and loan fully paid: reward = +(loan_amnt * int_rate)\\n    * If action == 1 and loan defaulted: reward = -loan_amnt\\n\\nWe use d3rlpy (a modern offline RL library) and the CQL (Conservative Q-Learning) algorithm.\\nWe estimate the learned policy value with Fitted Q Evaluation (FQE) implemented in d3rlpy.\\n\\nHow to use on Kaggle\\n---------------------\\n1. Upload dataset file to the Kaggle notebook (Data pane) and name it `lending_club_loan_two.csv`.\\n2. Run this notebook. It will: install dependencies, preprocess, create an offline dataset, train CQL, and compute Estimated Policy Value (EPV) using FQE.\\n\\nNotes\\n-----\\n- The script treats each loan instance as a single-step episode (terminal=True).\\n- The reward engineering follows the user's specification. You may tune reward scaling (e.g., divide by 1e3) to stabilize learning.\\n\\n\""},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# !pip install d3rlpy==2.8.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:33:36.862032Z","iopub.execute_input":"2025-10-30T10:33:36.862309Z","iopub.status.idle":"2025-10-30T10:33:41.305613Z","shell.execute_reply.started":"2025-10-30T10:33:36.862289Z","shell.execute_reply":"2025-10-30T10:33:41.304866Z"}},"outputs":[{"name":"stdout","text":"Collecting d3rlpy==2.8.0\n  Downloading d3rlpy-2.8.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: torch>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (2.6.0+cu124)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (4.67.1)\nRequirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (3.14.0)\nRequirement already satisfied: gym>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (0.26.2)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (8.3.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (4.15.0)\nRequirement already satisfied: structlog in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (25.5.0)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (0.4.6)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (0.6.7)\nCollecting gymnasium>=1.0.0 (from d3rlpy==2.8.0)\n  Downloading gymnasium-1.2.1-py3-none-any.whl.metadata (10.0 kB)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from d3rlpy==2.8.0) (1.2.2)\nRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.8.0) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.8.0) (3.1.1)\nRequirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym>=0.26.0->d3rlpy==2.8.0) (0.0.8)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=1.0.0->d3rlpy==2.8.0) (0.0.4)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (3.19.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (2025.9.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.0->d3rlpy==2.8.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.5.0->d3rlpy==2.8.0) (1.3.0)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->d3rlpy==2.8.0) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->d3rlpy==2.8.0) (0.9.0)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->d3rlpy==2.8.0) (1.15.3)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->d3rlpy==2.8.0) (1.5.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->d3rlpy==2.8.0) (3.6.0)\nRequirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.11/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->d3rlpy==2.8.0) (25.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->d3rlpy==2.8.0) (1.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.0->d3rlpy==2.8.0) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.0->gym>=0.26.0->d3rlpy==2.8.0) (2024.2.0)\nDownloading d3rlpy-2.8.0-py3-none-any.whl (201 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gymnasium-1.2.1-py3-none-any.whl (951 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m951.1/951.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: gymnasium, d3rlpy\n  Attempting uninstall: gymnasium\n    Found existing installation: gymnasium 0.29.0\n    Uninstalling gymnasium-0.29.0:\n      Successfully uninstalled gymnasium-0.29.0\n  Attempting uninstall: d3rlpy\n    Found existing installation: d3rlpy 2.3.0\n    Uninstalling d3rlpy-2.3.0:\n      Successfully uninstalled d3rlpy-2.3.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nstable-baselines3 2.1.0 requires gymnasium<0.30,>=0.28.1, but you have gymnasium 1.2.1 which is incompatible.\nkaggle-environments 1.18.0 requires gymnasium==0.29.0, but you have gymnasium 1.2.1 which is incompatible.\ndopamine-rl 4.1.2 requires gym<=0.25.2, but you have gym 0.26.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed d3rlpy-2.8.0 gymnasium-1.2.1\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ----------------------\n# Imports\n# ----------------------\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport category_encoders as ce\n\n# d3rlpy (offline RL)\nimport d3rlpy\nfrom d3rlpy.dataset import MDPDataset\nfrom d3rlpy.algos import CQL\nfrom d3rlpy.metrics import evaluate_on_dataset# For FQE\nfrom d3rlpy.dataset import Transition\n\n# ----------------------\n# Configuration\n# ----------------------\nCSV_PATH = '/kaggle/input/lending-club/accepted_2007_to_2018Q4.csv.gz'\n# If your CSV is at a different path, update CSV_PATH.\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\n\n# Reward scaling - recommended to stabilize training. If you want raw currency, set to 1.0\nREWARD_SCALE = 1.0  # e.g., 1e-3 to scale dollars to thousands\n\n# Train hyperparams (small by default for Kaggle CPU). Increase epochs on GPU.\nN_EPOCHS = 20\nBATCH_SIZE = 256\n\n# ----------------------\n# Load data\n# ----------------------\nprint('Loading CSV:', CSV_PATH)\nif not os.path.exists(CSV_PATH):\n    raise FileNotFoundError(f\"CSV not found at {CSV_PATH}. Upload the LendingClub CSV to the Kaggle notebook and update CSV_PATH.\")\n\ndf = pd.read_csv(CSV_PATH, low_memory=False)\nprint('Raw rows:', len(df))\n\n# ----------------------\n# Quick target engineering: map loan_status to binary outcome\n# ----------------------\n# We'll treat these as 'fully paid' vs 'charged off'/'default' etc.\n# Adapt based on your exact CSV's 'loan_status' values. Common values: Fully Paid, Charged Off, Late (31-120 days), Current, etc.\n\n# Keep only accepted statuses for our simple binary outcome\n\ndef map_loan_outcome(status):\n    s = str(status).lower()\n    if 'fully paid' in s:\n        return 'fully_paid'\n    if 'charged off' in s or 'default' in s:\n        return 'defaulted'\n    # treat late or other bad states as default for conservatism\n    if 'late' in s:\n        return 'defaulted'\n    # current / in progress - drop these rows because repayment outcome not observed\n    return 'other'\n\nif 'loan_status' not in df.columns:\n    raise ValueError('CSV does not contain loan_status column. Please check the dataset.')\n\nprint('Mapping loan_status to outcomes...')\ndf['outcome'] = df['loan_status'].apply(map_loan_outcome)\nprint('Outcome value counts (pre-filter):')\nprint(df['outcome'].value_counts())\n\n# Drop rows where outcome == 'other' (still in progress)\ndf = df[df['outcome'].isin(['fully_paid', 'defaulted'])].copy()\nprint('Rows after dropping in-progress loans:', len(df))\n\n# ----------------------\n# Select features\n# ----------------------\n# Choose a manageable set of features for the example. You can extend this.\nFEATURES = [\n    'loan_amnt', 'term', 'int_rate', 'installment', 'grade', 'sub_grade',\n    'emp_length', 'home_ownership', 'annual_inc', 'verification_status',\n    'purpose', 'addr_state', 'dti', 'delinq_2yrs', 'revol_util', 'open_acc',\n    'pub_rec', 'total_acc'\n]\n\nfor col in FEATURES:\n    if col not in df.columns:\n        print(f'Warning: column {col} not found in CSV; it will be skipped')\n\nFEATURES = [c for c in FEATURES if c in df.columns]\nprint('Using features:', FEATURES)\n\n# Target columns used for reward\nif 'loan_amnt' not in df.columns or 'int_rate' not in df.columns:\n    raise ValueError('Dataset must contain loan_amnt and int_rate columns for reward calculation.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:34:04.326058Z","iopub.execute_input":"2025-10-30T10:34:04.326670Z","iopub.status.idle":"2025-10-30T10:34:04.354364Z","shell.execute_reply.started":"2025-10-30T10:34:04.326644Z","shell.execute_reply":"2025-10-30T10:34:04.353527Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/4176408940.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0md3rlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMDPDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0md3rlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0md3rlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mevaluate_on_dataset\u001b[0m\u001b[0;31m# For FQE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0md3rlpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTransition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'evaluate_on_dataset' from 'd3rlpy.metrics' (/usr/local/lib/python3.11/dist-packages/d3rlpy/metrics/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'evaluate_on_dataset' from 'd3rlpy.metrics' (/usr/local/lib/python3.11/dist-packages/d3rlpy/metrics/__init__.py)","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"# ----------------------\n# Clean and preprocess\n# ----------------------\n# Preprocessing pipelines for numeric and categorical\nnumeric_features = [c for c in FEATURES if df[c].dtype in [np.float64, np.int64] or pd.api.types.is_numeric_dtype(df[c])]\ncategorical_features = [c for c in FEATURES if c not in numeric_features]\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler()),\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', ce.OneHotEncoder(handle_unknown='ignore', use_cat_names=True))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features),\n    ],\n    remainder='drop'\n)\n\nX_raw = df[FEATURES]\ny_raw = df['outcome'].map({'fully_paid': 1, 'defaulted': 0}).astype(int)\n\nprint('Preprocessing features...')\nX_proc = preprocessor.fit_transform(X_raw)\n# category_encoders returns pandas DataFrame for OneHotEncoder; ensure numpy array\nif hasattr(X_proc, 'toarray'):\n    X_proc = X_proc.toarray()\n\nprint('Processed feature shape:', X_proc.shape)\n\n# ----------------------\n# Build offline dataset (one-step episodes)\n# ----------------------\n# Actions: in the dataset we don't have decisions made by a logging policy we can use.\n# For offline RL training we need transitions (s,a,r,s',terminal).\n# Here we will construct a dataset from the logged decisions by synthetic logging policy:\n# Option A: Use the historical decision (if available) — e.g., whether loan was approved by LendingClub.\n# Option B: If there's no 'approved' column, we can treat the dataset as full offline context-action-reward by\n# generating actions using the historical origination (application accepted) column 'loan_status' or 'is_approved' if present.\n\n# Commonly LendingClub dataset contains 'verification_status' and 'loan_status' but not a direct 'action' column.\n# For this pipeline, we'll assume that rows represent *approved loans* (i.e., applicants that were approved and funded),\n# and therefore we do not observe rewards for denied actions. This is a classical selection bias issue.\n# For demonstration, we'll create a synthetic logging policy: a simple rule-based policy that approves if grade in [A,B,C] and income>some threshold.\n\n# You may replace the synthetic logging policy with the dataset's true origination indicator if available (e.g., 'funded_amnt' > 0)\n\nprint('Creating a synthetic logging policy to generate logged actions...')\n\n# Synthetic logging policy (0 deny, 1 approve)\ndef synthetic_policy_row(row):\n    # Approve if grade A/B/C or annual_inc > 50000\n    grade = row.get('grade')\n    income = row.get('annual_inc', 0)\n    if pd.isna(grade):\n        return 0\n    if grade in ['A', 'B', 'C']:\n        return 1\n    if income >= 50000:\n        return 1\n    return 0\n\n# Apply to original df (use raw df to access grade/annual_inc)\ndf['logged_action'] = df.apply(synthetic_policy_row, axis=1)\nprint('Logged action distribution (synthetic):')\nprint(df['logged_action'].value_counts())\n\n# Compute reward per user-specified spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:59.592167Z","iopub.status.idle":"2025-10-30T10:21:59.592469Z","shell.execute_reply.started":"2025-10-30T10:21:59.592330Z","shell.execute_reply":"2025-10-30T10:21:59.592346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_reward(row, reward_scale=REWARD_SCALE):\n    a = row['action']\n    if a == 0:\n        return 0.0\n    loan_amnt = float(row['loan_amnt'])\n    # int_rate may be like '13.56%' — try to parse\n    ir = row['int_rate']\n    if isinstance(ir, str) and '%' in ir:\n        try:\n            ir_val = float(ir.strip().replace('%',''))/100.0\n        except:\n            ir_val = 0.0\n    else:\n        ir_val = float(ir)\n    if row['outcome'] == 'fully_paid':\n        return (loan_amnt * ir_val) / reward_scale\n    else:\n        return (-loan_amnt) / reward_scale\n\n# For training dataset, we'll use actions from logged_action\ntrain_df = df.copy()\ntrain_df['action'] = train_df['logged_action']\ntrain_df['reward'] = train_df.apply(lambda r: compute_reward(r), axis=1)\n\n# Build arrays for d3rlpy MDPDataset. Since these are one-step, next_state can be zeros and terminal True.\nobservations = X_proc.astype('float32')\nactions = train_df['action'].values.astype('int32')\nrewards = train_df['reward'].values.astype('float32')\nterminals = np.ones_like(rewards, dtype=bool)  # every transition terminal (single-step episode)\nnext_observations = np.zeros_like(observations)\n\nprint('Observations shape:', observations.shape)\nprint('Actions shape:', actions.shape)\nprint('Rewards stats: min %.3f, mean %.3f, max %.3f' % (rewards.min(), rewards.mean(), rewards.max()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:59.593679Z","iopub.status.idle":"2025-10-30T10:21:59.594009Z","shell.execute_reply.started":"2025-10-30T10:21:59.593855Z","shell.execute_reply":"2025-10-30T10:21:59.593873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create MDPDataset\nmdp_dataset = MDPDataset(observations=observations,\n                         actions=actions.reshape(-1,1),\n                         rewards=rewards.reshape(-1,1),\n                         terminals=terminals,\n                         next_observations=next_observations)\n\n# Split train / eval for training the offline RL algorithm and for FQE\nobs_train, obs_eval, a_train, a_eval, r_train, r_eval = train_test_split(\n    observations, actions, rewards, test_size=0.2, random_state=RANDOM_SEED)\n\nmdp_train = MDPDataset(observations=obs_train,\n                       actions=a_train.reshape(-1,1),\n                       rewards=r_train.reshape(-1,1),\n                       terminals=np.ones_like(r_train, dtype=bool),\n                       next_observations=np.zeros_like(obs_train))\n\nmdp_eval = MDPDataset(observations=obs_eval,\n                      actions=a_eval.reshape(-1,1),\n                      rewards=r_eval.reshape(-1,1),\n                      terminals=np.ones_like(r_eval, dtype=bool),\n                      next_observations=np.zeros_like(obs_eval))\n\nprint('MDP train size:', len(mdp_train.observations))\nprint('MDP eval size:', len(mdp_eval.observations))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:59.595561Z","iopub.status.idle":"2025-10-30T10:21:59.595805Z","shell.execute_reply.started":"2025-10-30T10:21:59.595700Z","shell.execute_reply":"2025-10-30T10:21:59.595711Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------\n# Train an offline RL algorithm: CQL\n# ----------------------\n# We'll use d3rlpy.CQL\n\nobs_dim = mdp_train.observations.shape[1]\nprint('Observation dim:', obs_dim)\n\ncql = CQL(\n    actor_learning_rate=3e-4,\n    critic_learning_rate=3e-4,\n    batch_size=BATCH_SIZE,\n    n_critics=2,\n    # conservative weight controls regularization - tune as necessary\n    conservative_weight=5.0,\n    q_func_factory=d3rlpy.models.builders.create_q_func,\n    scaler='standard'\n)\n\n# Fit the model\nprint('Starting CQL training...')\ncql.fit(mdp_train,\n        n_epochs=N_EPOCHS,\n        eval_episodes=mdp_eval,\n        scorers={'average_value': average_value_estimation_scorer},\n        verbose=True)\n\n# Save the trained model\nMODEL_DIR = '/kaggle/working/cql_model'\nos.makedirs(MODEL_DIR, exist_ok=True)\ncql.save_model(MODEL_DIR)\nprint('Saved model to', MODEL_DIR)\n\n# ----------------------\n# Policy extraction: deterministic greedy policy from learned Q\n# ----------------------\npolicy = cql.predict_policy\n\n# You can use cql.predict to get actions for observations\nsample_obs = mdp_eval.observations[:10]\npred_actions = cql.predict(sample_obs)\nprint('Sample predicted actions:', pred_actions)\n\n# ----------------------\n# Estimate policy value (Estimated Policy Value, EPV) using FQE\n# ----------------------\n# Fitted Q Evaluation trains a Q-function off-policy and estimates expected return of the evaluation policy.\nfrom d3rlpy.metrics import fqe\n\nprint('Running FQE for policy value estimation...')\n\n# Create an evaluation policy wrapper for d3rlpy's functions\nclass SimplePolicy:\n    def __init__(self, model):\n        self.model = model\n    def predict(self, x: np.ndarray) -> np.ndarray:\n        # d3rlpy's algo.predict returns action indices\n        return self.model.predict(x.astype('float32'))\n\neval_policy = SimplePolicy(cql)\n\n# Fit FQE on the training dataset and evaluate the policy\n# d3rlpy provides FQE implementation via fqe module\n\nfqe_agent = d3rlpy.algos.FQE(\n    batch_size=256,\n    n_steps=500,\n    gamma=0.99,\n    q_func_factory=d3rlpy.models.builders.create_q_func,\n    scaler='standard'\n)\n\n# Combine train and eval to form an experience replay for FQE\nfull_mdp = mdp_dataset\n\n# Fit FQE on the logged data\nfqe_agent.fit(full_mdp, n_epochs=10)\n\n# Estimate policy value by calling fqe_agent.estimate_return\nestimated_return = fqe_agent.estimate_return(full_mdp, eval_policy)\nprint('Estimated policy value (per-episode return):', estimated_return)\n\n# If you used reward_scale !=1.0, multiply back to get dollar amounts\nestimated_return_dollars = estimated_return * REWARD_SCALE\nprint('Estimated policy value (dollars per decision):', estimated_return_dollars)\n\n# ----------------------\n# Logging & checkpoints\n# ----------------------\n# Save evaluation figures and metrics\nmetrics_df = pd.DataFrame({\n    'estimated_return': [float(estimated_return)],\n    'estimated_return_dollars': [float(estimated_return_dollars)],\n})\nmetrics_df.to_csv('/kaggle/working/epv_metrics.csv', index=False)\nprint('Saved EPV metrics to /kaggle/working/epv_metrics.csv')\n\nprint('\\nDone. To get final numbers, run this notebook on Kaggle (GPU recommended).')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-30T10:21:59.597126Z","iopub.status.idle":"2025-10-30T10:21:59.597436Z","shell.execute_reply.started":"2025-10-30T10:21:59.597279Z","shell.execute_reply":"2025-10-30T10:21:59.597293Z"}},"outputs":[],"execution_count":null}]}